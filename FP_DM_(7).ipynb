{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUwbEN1MpMCb"
      },
      "source": [
        "# **Melakukan scraping data pada Trip.com**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cj5mUPfZpVZT"
      },
      "outputs": [],
      "source": [
        "# Install library yang dibutuhkan\n",
        "!pip install google-play-scraper\n",
        "!pip install pandas numpy scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1ndPek0ppwD"
      },
      "outputs": [],
      "source": [
        "# Import library\n",
        "from google_play_scraper import Sort, reviews\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTTqTenvptoE"
      },
      "outputs": [],
      "source": [
        "# Scraping data ulasan\n",
        "review, continuation_token = reviews(\n",
        "    'ctrip.english',                    # ID aplikasi Trip.com\n",
        "    lang='id',                          # Bahasa ulasan (Indonesia)\n",
        "    country='id',                       # Negara\n",
        "    sort=Sort.MOST_RELEVANT,            # Sortir berdasarkan relevansi\n",
        "    count=300                           # Jumlah ulasan yang ingin diambil\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCsnZ_v0pxMQ"
      },
      "outputs": [],
      "source": [
        "# Simpan data dalam DataFrame\n",
        "comments = pd.DataFrame(review)\n",
        "comments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsXA3N_Vp9qE"
      },
      "outputs": [],
      "source": [
        "# Simpan ke CSV\n",
        "comments.to_csv('tripcom-review.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sGpKJBrACrl"
      },
      "source": [
        "# ***Exploratory Data Analysis* (EDA)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-mX5jxfABse"
      },
      "outputs": [],
      "source": [
        "# Import library\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dGNsAMPAWKC"
      },
      "outputs": [],
      "source": [
        "# Informasi umum tentang dataset\n",
        "comments_cleaned.info()\n",
        "\n",
        "# Cek jumlah nilai yang hilang (missing values)\n",
        "comments_cleaned.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7CVD2WmAigz"
      },
      "outputs": [],
      "source": [
        "# Distribusi skor ulasan\n",
        "sns.countplot(x='score', data=comments_cleaned, palette='viridis')\n",
        "plt.title('Distribusi Skor Ulasan')\n",
        "plt.xlabel('Skor Ulasan')\n",
        "plt.ylabel('Jumlah Ulasan')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1cPWEsZAmlO"
      },
      "outputs": [],
      "source": [
        "# Tambahkan kolom jumlah karakter pada setiap ulasan\n",
        "comments_cleaned['content_length'] = comments_cleaned['content'].apply(len)\n",
        "\n",
        "# Visualisasi distribusi panjang konten ulasan\n",
        "sns.histplot(comments_cleaned['content_length'], bins=30, kde=True, color='blue')\n",
        "plt.title('Distribusi Panjang Ulasan')\n",
        "plt.xlabel('Panjang (Jumlah Karakter)')\n",
        "plt.ylabel('Frekuensi')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mV7kyMFAqro"
      },
      "outputs": [],
      "source": [
        "# Import library tambahan\n",
        "import re\n",
        "from wordcloud import WordCloud\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Bersihkan teks (hilangkan simbol, angka, dll.)\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^A-Za-z\\s]', '', text) # Hapus karakter non-huruf\n",
        "    text = text.lower()                     # Ubah ke huruf kecil\n",
        "    return text\n",
        "\n",
        "# Terapkan pembersihan pada kolom 'content'\n",
        "comments_cleaned['cleaned_content'] = comments_cleaned['content'].apply(clean_text)\n",
        "\n",
        "# Gabungkan semua ulasan menjadi satu string\n",
        "all_words = ' '.join([text for text in comments_cleaned['cleaned_content']])\n",
        "\n",
        "# Buat WordCloud\n",
        "wordcloud = WordCloud(stopwords=set(stopwords.words('indonesian')),\n",
        "                      background_color='white',\n",
        "                      max_words=100,\n",
        "                      width=800,\n",
        "                      height=400).generate(all_words)\n",
        "\n",
        "# Visualisasikan WordCloud\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Kata yang sering muncul')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aftHemLAwCw"
      },
      "outputs": [],
      "source": [
        "# Visualisasi jumlah ulasan yang puas dan tidak puas (berdasarkan skor)\n",
        "comments_cleaned['sentimen'] = np.where(comments_cleaned['score'] >= 4, 'Puas', 'Tidak Puas')\n",
        "\n",
        "# Visualisasi distribusi sentimen\n",
        "sns.countplot(x='sentimen', data=comments_cleaned, palette='pastel')\n",
        "plt.title('Distribusi Sentimen Ulasan (Berdasarkan Skor)')\n",
        "plt.xlabel('Sentimen')\n",
        "plt.ylabel('Jumlah Ulasan')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyXJ2v7oqOeu"
      },
      "source": [
        "# **Cleaning Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4w2QCH8qqRfw"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Hapus kolom yang tidak diperlukan\n",
        "comments_cleaned = comments[['content', 'score']]  # Hanya ambil kolom 'content' dan 'Skor'\n",
        "\n",
        "# Hapus komentar yang kosong\n",
        "comments_cleaned = comments_cleaned.dropna(subset=['content', 'score'])\n",
        "\n",
        "# Menghapus karakter khusus (tanda baca), angka, dan URL\n",
        "comments_cleaned['content'] = comments_cleaned['content'].apply(lambda x: re.sub(r'http\\S+|www\\S+|\\S+@\\S+', '', x))  # Hapus URL\n",
        "comments_cleaned['content'] = comments_cleaned['content'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x))  # Hapus karakter selain huruf\n",
        "\n",
        "# Hapus komentar yang kosong\n",
        "comments_cleaned = comments_cleaned.dropna(subset=['content'])\n",
        "\n",
        "# Tampilkan beberapa contoh setelah data cleaning\n",
        "comments_cleaned.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T36-aQ0qqYGq"
      },
      "source": [
        "# **Case Folding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfSYKGZXqiIL"
      },
      "outputs": [],
      "source": [
        "# Terapkan case folding untuk mengubah semua teks menjadi huruf kecil\n",
        "comments_cleaned['content'] = comments_cleaned['content'].str.lower()\n",
        "\n",
        "# Tampilkan beberapa contoh setelah case folding\n",
        "comments_cleaned.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atEx24fPxax-"
      },
      "source": [
        "#**Tokenisasi**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltSNoMHZ74AW"
      },
      "outputs": [],
      "source": [
        "# Tokenisasi (Pisahkan Teks menjadi Daftar Kata)\n",
        "comments_cleaned['tokens'] = comments_cleaned['content'].apply(word_tokenize)\n",
        "comments_cleaned.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30gZUrmu11ZI"
      },
      "source": [
        "# **Penghapusan STOPWORD**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dV0YKfaiq_5q"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize  # Impor word_tokenize\n",
        "from nltk.corpus import stopwords  # Impor stopwords\n",
        "nltk.data.clear_cache()\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1D2vWmG9G0i"
      },
      "outputs": [],
      "source": [
        "# Hapus Stopwords (Hapus Kata-kata Umum)\n",
        "try:\n",
        "    stop_words = set(stopwords.words('indonesian'))  # Daftar stopwords bahasa Indonesia\n",
        "except OSError:\n",
        "    print(\"Stopwords bahasa Indonesia tidak tersedia di NLTK. Gunakan pustaka Sastrawi atau masukkan stopwords secara manual.\")\n",
        "    stop_words = set()  # Jika tidak ada stopwords Indonesia, gunakan set kosong\n",
        "\n",
        "comments_cleaned['tokens_no_stopwords'] = comments_cleaned['tokens'].apply(lambda x: [word for word in x if word.lower() not in stop_words])  # Hapus stopwords\n",
        "\n",
        "comments_cleaned.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDrV9BsP2BdU"
      },
      "source": [
        "# **STEMMING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnZNfefF-q1x"
      },
      "outputs": [],
      "source": [
        "#Stemming data\n",
        "!pip install Sastrawi\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "\n",
        "# Inisialisasi Stemmer\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "comments_cleaned['stemmed_tokens'] = comments_cleaned['tokens_no_stopwords'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
        "\n",
        "comments_cleaned.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVhnThVO2Hzl"
      },
      "source": [
        "# **LABELING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2HkKqXcAxRf"
      },
      "outputs": [],
      "source": [
        "#Pelabelan\n",
        "def label_based_on_score(score):\n",
        "    \"\"\"Buat label puas atau tidak puas berdasarkan skor ulasan.\"\"\"\n",
        "    if score >= 4:\n",
        "        return 1  # Puas\n",
        "    elif score <= 3:\n",
        "        return 0  # Tidak Puas\n",
        "\n",
        "comments_cleaned['sentiment_label'] = comments_cleaned['score'].apply(label_based_on_score)\n",
        "\n",
        "# comments_cleaned['sentiment'] = comments_cleaned['content'].apply(label_sentiment)\n",
        "comments_cleaned.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFOJK_bd_0wR"
      },
      "outputs": [],
      "source": [
        "#Simpan ke CSV\n",
        "comments_cleaned.to_csv('cleaned_comments_tripcom.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pw54lo-2VkI"
      },
      "source": [
        "# **VECTORISASI**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haizDyunIqy4"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "# TF-IDF Vectorizer (Ubah teks ke dalam bentuk vektor numerik)\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X = tfidf_vectorizer.fit_transform(comments_cleaned['content'])  # Fitur (X)\n",
        "y = comments_cleaned['sentiment_label']  # Label (y)\n",
        "\n",
        "#Fitur(Kata-kata unik) dari TF-IDF\n",
        "tfidf_vectorizer.get_feature_names_out()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cn-RSGQOJrif"
      },
      "outputs": [],
      "source": [
        "tfidf_matrix = X.toarray()\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix, columns=tfidf_vectorizer.get_feature_names_out())\n",
        "tfidf_df.head(10)\n",
        "# tfidf_df.to_csv('tfidf_matrix.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dF7D-ri2dum"
      },
      "source": [
        "# **SPLITTING DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQqhLYMPG3Nz"
      },
      "outputs": [],
      "source": [
        "#Skenario 1# data latih (80%) dan data uji (20%)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Memisahkan fitur dan target\n",
        "X = comments_cleaned['content']  # Ulasan yang akan dianalisis\n",
        "y = comments_cleaned['sentiment_label']  # Sentimen yang sudah dilabeli\n",
        "\n",
        "# Membagi data menjadi data latih (80%) dan data uji (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Menampilkan bentuk data latih dan data uji\n",
        "print(\"Data Latih:\", X_train.shape)\n",
        "print(\"Data Uji:\", X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJ7gRPaRcvXk"
      },
      "outputs": [],
      "source": [
        "#Skenario 2# data latih (60%) dan data uji (40%)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Memisahkan fitur dan target\n",
        "X = comments_cleaned['content']  # Ulasan yang akan dianalisis\n",
        "y = comments_cleaned['sentiment_label']  # Sentimen yang sudah dilabeli\n",
        "\n",
        "# Membagi data menjadi data latih (60%) dan data uji (40%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "\n",
        "# Menampilkan bentuk data latih dan data uji\n",
        "print(\"Data Latih:\", X_train.shape)\n",
        "print(\"Data Uji:\", X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bl797cb38pc"
      },
      "source": [
        "# **UJI MODEL**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RS7GeSWq4bim"
      },
      "source": [
        "**DecissionTree**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qeyh3sbOdTEm"
      },
      "outputs": [],
      "source": [
        "#DecisionTree\n",
        "\n",
        "import joblib\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Model Decision Tree\n",
        "model_dt = DecisionTreeClassifier(random_state=42)\n",
        "model_dt.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Prediksi dan evaluasi\n",
        "y_pred_dt = model_dt.predict(X_test_tfidf)\n",
        "print(\"Decision Tree - Accuracy:\", accuracy_score(y_test, y_pred_dt))\n",
        "print(\"Decision Tree - Classification Report:\\n\", classification_report(y_test, y_pred_dt))\n",
        "\n",
        "# Simpan Model dan TF-IDF ke File\n",
        "joblib.dump(model_dt, 'decision_tree_model.pkl')  # Menyimpan model Decision Tree\n",
        "joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')  # Menyimpan vektorisator\n",
        "print(\"Model Decision Tree dan TF-IDF berhasil disimpan!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdyBGv-W4el4"
      },
      "source": [
        "**Logistic**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vh2vFj7nKRAX"
      },
      "outputs": [],
      "source": [
        "#Logistic\n",
        "\n",
        "import joblib\n",
        "from sklearn.linear_model import LogisticRegression  # Import Logistic Regression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Model Logistic Regression\n",
        "model_lr = LogisticRegression()  # Menggunakan Logistic Regression\n",
        "model_lr.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Prediksi dan evaluasi\n",
        "y_pred_lr = model_lr.predict(X_test_tfidf)\n",
        "print(\"Logistic Regression - Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
        "print(\"Logistic Regression - Classification Report:\\n\", classification_report(y_test, y_pred_lr))\n",
        "\n",
        "# Simpan Model dan TF-IDF ke File\n",
        "joblib.dump(model_lr, 'logistic_model.pkl')  # Menyimpan model Logistic Regression\n",
        "joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')  # Menyimpan vektorisator\n",
        "print(\"Model Logistic Regression dan TF-IDF berhasil disimpan!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dph6_ZIk4i9u"
      },
      "source": [
        "**RandomForest**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tg4DNm1vZzPT"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "from sklearn.ensemble import RandomForestClassifier  # Import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Model Random Forest\n",
        "model_rf = RandomForestClassifier(n_estimators=100, random_state=42)  # Menggunakan Random Forest\n",
        "model_rf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Prediksi dan evaluasi\n",
        "y_pred_rf = model_rf.predict(X_test_tfidf)\n",
        "print(\"Random Forest - Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
        "print(\"Random Forest - Classification Report:\\n\", classification_report(y_test, y_pred_rf))\n",
        "\n",
        "# Simpan Model dan TF-IDF ke File\n",
        "joblib.dump(model_rf, 'random_forest_model.pkl')  # Menyimpan model Random Forest\n",
        "joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')  # Menyimpan vektorisator\n",
        "print(\"Model Random Forest dan TF-IDF berhasil disimpan!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T65vqEaI4n6f"
      },
      "source": [
        "**Naive Bayes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YztOfbxYTj_T"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "from sklearn.naive_bayes import MultinomialNB  # Import Multinomial Naive Bayes\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Model Naive Bayes\n",
        "model_nb = MultinomialNB()  # Menggunakan Naive Bayes\n",
        "model_nb.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Prediksi dan evaluasi\n",
        "y_pred_nb = model_nb.predict(X_test_tfidf)\n",
        "print(\"Naive Bayes - Accuracy:\", accuracy_score(y_test, y_pred_nb))\n",
        "print(\"Naive Bayes - Classification Report:\\n\", classification_report(y_test, y_pred_nb))\n",
        "\n",
        "# Simpan Model dan TF-IDF ke File\n",
        "joblib.dump(model_nb, 'naive_bayes_model.pkl')  # Menyimpan model Naive Bayes\n",
        "joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')  # Menyimpan vektorisator\n",
        "print(\"Model Naive Bayes dan TF-IDF berhasil disimpan!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkIXkgK88pQJ"
      },
      "source": [
        "# **DEPLOYMENT TO STREAMLIT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6sKOl8G7V3F"
      },
      "outputs": [],
      "source": [
        "!pip install streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzuiB051CZyw",
        "outputId": "2a20ef80-c28b-4882-fa7f-5cf1871a2361"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  # Gunakan TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from google_play_scraper import reviews, Sort\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Memuat model dan vektorisator\n",
        "model = joblib.load('naive_bayes_model.pkl')  # Pastikan menggunakan model yang benar\n",
        "vectorizer = joblib.load('tfidf_vectorizer.pkl')  # Memuat vektorisator yang sesuai\n",
        "\n",
        "# Fungsi untuk prediksi sentimen\n",
        "def predict_sentiment(text):\n",
        "    # Mengubah teks input menjadi fitur numerik menggunakan TF-IDF\n",
        "    text_tfidf = vectorizer.transform([text])\n",
        "    # Prediksi sentimen\n",
        "    sentiment = model.predict(text_tfidf)[0]\n",
        "    return sentiment\n",
        "\n",
        "# Fungsi pembersihan teks\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^A-Za-z\\s]', '', text)  # Hapus karakter non-huruf\n",
        "    text = text.lower()  # Ubah ke huruf kecil\n",
        "    return text\n",
        "\n",
        "# **EDA Fungsi**\n",
        "def perform_eda(data):\n",
        "    st.subheader('1. Informasi Data')\n",
        "    st.write(data.head())\n",
        "    st.write(\"Jumlah Data:\", data.shape[0])\n",
        "    st.write(\"Jumlah Kolom:\", data.shape[1])\n",
        "    st.write(data.info())\n",
        "\n",
        "    st.subheader('2. Distribusi Skor Ulasan')\n",
        "    fig, ax = plt.subplots()\n",
        "    sns.countplot(x='score', data=data, palette='viridis', ax=ax)\n",
        "    ax.set_title('Distribusi Skor Ulasan')\n",
        "    ax.set_xlabel('Skor Ulasan')\n",
        "    ax.set_ylabel('Jumlah Ulasan')\n",
        "    st.pyplot(fig)\n",
        "\n",
        "    st.subheader('3. Distribusi Panjang Ulasan')\n",
        "    data['content_length'] = data['content'].apply(len)\n",
        "    fig, ax = plt.subplots()\n",
        "    sns.histplot(data['content_length'], bins=30, kde=True, color='blue', ax=ax)\n",
        "    ax.set_title('Distribusi Panjang Ulasan')\n",
        "    ax.set_xlabel('Panjang Ulasan (Jumlah Karakter)')\n",
        "    ax.set_ylabel('Frekuensi')\n",
        "    st.pyplot(fig)\n",
        "\n",
        "    st.subheader('4. WordCloud Kata Dominan')\n",
        "    all_words = ' '.join([clean_text(text) for text in data['content']])\n",
        "    wordcloud = WordCloud(stopwords=set(stopwords.words('indonesian')),\n",
        "                          background_color='white',\n",
        "                          max_words=100,\n",
        "                          width=800,\n",
        "                          height=400).generate(all_words)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    st.pyplot()\n",
        "\n",
        "    st.subheader('5. Sentimen Berdasarkan Skor')\n",
        "    data['sentimen'] = np.where(data['score'] >= 4, 'Puas', 'Tidak Puas')\n",
        "    fig, ax = plt.subplots()\n",
        "    sns.countplot(x='sentimen', data=data, palette='pastel', ax=ax)\n",
        "    ax.set_title('Distribusi Sentimen (Berdasarkan Skor)')\n",
        "    ax.set_xlabel('Sentimen')\n",
        "    ax.set_ylabel('Jumlah Ulasan')\n",
        "    st.pyplot(fig)\n",
        "\n",
        "#FUNGSI PREPOCESSING DATA\n",
        "\n",
        "# Inisialisasi Stemmer Sastrawi\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "# **Fungsi Pembersihan Data**\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'http\\S+|www\\S+|\\S+@\\S+', '', text)  # Hapus URL dan email\n",
        "    text = re.sub(r'[^A-Za-z\\s]', '', text)  # Hapus karakter selain huruf\n",
        "    text = text.lower()  # Ubah ke huruf kecil (case folding)\n",
        "    return text\n",
        "\n",
        "# **Fungsi Tokenisasi**\n",
        "def tokenize_text(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "# **Fungsi untuk Hapus Stopwords**\n",
        "def remove_stopwords(tokens):\n",
        "    stop_words = set(stopwords.words('indonesian'))  # Stopwords bahasa Indonesia\n",
        "    return [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "# **Fungsi untuk Stemming**\n",
        "def stem_tokens(tokens):\n",
        "    return [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "# **Fungsi untuk Memberikan Label Sentimen**\n",
        "def label_based_on_score(score):\n",
        "    \"\"\"Buat label puas atau tidak puas berdasarkan skor ulasan.\"\"\"\n",
        "    if score >= 4:\n",
        "        return 1  # Puas\n",
        "    elif score <= 3:\n",
        "        return 0  # Tidak Puas\n",
        "\n",
        "# **Fungsi Preprocessing**\n",
        "def preprocess_data(data):\n",
        "    # Hanya ambil kolom 'content' dan 'score'\n",
        "    if 'content' in data.columns and 'score' in data.columns:\n",
        "        comments_cleaned = data[['content', 'score']].copy()\n",
        "    else:\n",
        "        st.error(\"File CSV harus memiliki kolom 'content' dan 'score'.\")\n",
        "        return None\n",
        "\n",
        "    # Hapus nilai kosong\n",
        "    comments_cleaned = comments_cleaned.dropna(subset=['content', 'score'])\n",
        "\n",
        "    # Pembersihan teks (URL, karakter khusus)\n",
        "    comments_cleaned['content'] = comments_cleaned['content'].apply(clean_text)\n",
        "\n",
        "    # Tokenisasi\n",
        "    comments_cleaned['tokens'] = comments_cleaned['content'].apply(tokenize_text)\n",
        "\n",
        "    # Hapus stopwords\n",
        "    comments_cleaned['tokens_no_stopwords'] = comments_cleaned['tokens'].apply(remove_stopwords)\n",
        "\n",
        "    # Stemming\n",
        "    comments_cleaned['stemmed_tokens'] = comments_cleaned['tokens_no_stopwords'].apply(stem_tokens)\n",
        "\n",
        "    # Memberikan label berdasarkan skor\n",
        "    comments_cleaned['sentiment_label'] = comments_cleaned['score'].apply(label_based_on_score)\n",
        "\n",
        "\n",
        "    # Proses TF-IDF Vectorization\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "    X = tfidf_vectorizer.fit_transform(comments_cleaned['content'])  # Fitur (X)\n",
        "    y = comments_cleaned['sentiment_label']  # Label (y)\n",
        "\n",
        "    # Simpan ke DataFrame\n",
        "    tfidf_matrix = X.toarray()\n",
        "    tfidf_df = pd.DataFrame(tfidf_matrix, columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "    return comments_cleaned, tfidf_df\n",
        "\n",
        "# **Fungsi untuk Split Data, Latih Model, dan Evaluasi**\n",
        "def train_and_evaluate_model(data):\n",
        "    X = data['content']\n",
        "    y = data['sentiment_label']\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "    X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "    model_nb = MultinomialNB()\n",
        "    model_nb.fit(X_train_tfidf, y_train)\n",
        "    y_pred_nb = model_nb.predict(X_test_tfidf)\n",
        "    acc_nb = accuracy_score(y_test, y_pred_nb)\n",
        "    report_nb = classification_report(y_test, y_pred_nb)\n",
        "    joblib.dump(model_nb, 'naive_bayes_model.pkl')\n",
        "\n",
        "    model_lr = LogisticRegression(max_iter=1000)\n",
        "    model_lr.fit(X_train_tfidf, y_train)\n",
        "    y_pred_lr = model_lr.predict(X_test_tfidf)\n",
        "    acc_lr = accuracy_score(y_test, y_pred_lr)\n",
        "    report_lr = classification_report(y_test, y_pred_lr)\n",
        "    joblib.dump(model_lr, 'logistic_regression_model.pkl')\n",
        "\n",
        "    model_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    model_rf.fit(X_train_tfidf, y_train)\n",
        "    y_pred_rf = model_rf.predict(X_test_tfidf)\n",
        "    acc_rf = accuracy_score(y_test, y_pred_rf)\n",
        "    report_rf = classification_report(y_test, y_pred_rf)\n",
        "    joblib.dump(model_rf, 'random_forest_model.pkl')\n",
        "\n",
        "    joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')\n",
        "\n",
        "    return {\n",
        "        'Naive Bayes': {'accuracy': acc_nb, 'report': report_nb},\n",
        "        'Logistic Regression': {'accuracy': acc_lr, 'report': report_lr},\n",
        "        'Random Forest': {'accuracy': acc_rf, 'report': report_rf}\n",
        "    }\n",
        "#__________________________________________________________________________________________________________________________________________________________________________________________________\n",
        "\n",
        "# **Streamlit App Layout**\n",
        "st.title('Sentiment Analysis & EDA')\n",
        "st.write(\"Masukkan ulasan untuk menganalisis sentimen atau unggah file CSV untuk melakukan EDA.\")\n",
        "\n",
        "# **Tab Navigasi**\n",
        "tab1, tab2, tab3, tab4, tab5 = st.tabs([\"📊 EDA\", \"📈 Prediksi Sentimen\", \"Scapping\", \"Preprocessing Data\", \"Training\"])\n",
        "\n",
        "# **TAB 1: EDA**\n",
        "with tab1:\n",
        "    st.header('📊 Eksplorasi Data Ulasan (EDA)')\n",
        "    uploaded_file = st.file_uploader(\"Unggah file CSV untuk EDA\", type=[\"csv\"])\n",
        "\n",
        "    if uploaded_file is not None:\n",
        "        # Membaca file CSV\n",
        "        data = pd.read_csv(uploaded_file)\n",
        "\n",
        "        # Pastikan hanya kolom 'content' dan 'score' yang diambil\n",
        "        if 'content' in data.columns and 'score' in data.columns:\n",
        "            perform_eda(data[['content', 'score']])\n",
        "        else:\n",
        "            st.error(\"File CSV harus memiliki kolom 'content' dan 'score'.\")\n",
        "    else:\n",
        "        st.write(\"Unggah file CSV untuk memulai EDA.\")\n",
        "\n",
        "# **TAB 2: Prediksi Sentimen**\n",
        "with tab2:\n",
        "    st.header('📈 Prediksi Sentimen')\n",
        "\n",
        "    # Input teks dari pengguna\n",
        "    user_input = st.text_area(\"Masukkan teks ulasan\")\n",
        "\n",
        "    # Jika pengguna mengklik tombol, tampilkan prediksi\n",
        "    if st.button('Prediksi Sentimen'):\n",
        "        if user_input:\n",
        "            sentiment = predict_sentiment(user_input)\n",
        "            if sentiment == 1:\n",
        "                st.success(\"Sentimen: **Puas** 😄\")\n",
        "            else:\n",
        "                st.error(\"Sentimen: **Tidak Puas** 😞\")\n",
        "        else:\n",
        "            st.warning(\"Tolong masukkan teks untuk analisis.\")\n",
        "\n",
        "#**TAB 3: Scrapping**\n",
        "with tab3:\n",
        "    st.header(\"Scraping Ulasan Google Play Store\")\n",
        "    app_id = st.text_input(\"Masukkan ID Aplikasi Google Play\", 'ctrip.english')\n",
        "    jumlah_ulasan = st.number_input(\"Jumlah ulasan yang ingin diambil\", min_value=10, max_value=1000, value=300, step=10)\n",
        "\n",
        "    if st.button('Scrape Data Ulasan'):\n",
        "        if app_id:\n",
        "            with st.spinner('Mengambil ulasan dari Google Play Store...'):\n",
        "                try:\n",
        "                    review, _ = reviews(\n",
        "                        app_id=app_id,\n",
        "                        lang='id',\n",
        "                        country='id',\n",
        "                        count=int(jumlah_ulasan),  # Pastikan jumlah ulasan adalah integer\n",
        "                        sort=Sort.MOST_RELEVANT  # Gunakan Sort.MOST_RELEVANT bukan string\n",
        "                    )\n",
        "                    # Validasi data yang diambil\n",
        "                    if isinstance(review, str):  # Jika review adalah string, berarti ada kesalahan\n",
        "                        st.error(f\"Terjadi kesalahan saat scraping: {review}\")\n",
        "                    else:\n",
        "                        data = pd.DataFrame(review)\n",
        "                        st.success(f\"Berhasil mengambil ulasan dari Google Play Store!\")\n",
        "\n",
        "                        # Tampilkan DataFrame\n",
        "                        st.write(\"📋 **Tampilan Data**\")\n",
        "                        st.dataframe(data.head(10))  # Menampilkan 10 data pertama\n",
        "\n",
        "                        # Simpan file CSV\n",
        "                        st.download_button(\n",
        "                            label=\"📁 Unduh Data Ulasan Sebagai CSV\",\n",
        "                            data=data.to_csv(index=False),\n",
        "                            file_name='ulasan_google_play.csv',\n",
        "                            mime='text/csv',\n",
        "                        )\n",
        "\n",
        "                        # Simpan data ke session state\n",
        "                        st.session_state['scraped_data'] = data\n",
        "\n",
        "                except Exception as e:\n",
        "                    st.error(f\"Terjadi kesalahan saat scraping: {e}\")\n",
        "        else:\n",
        "            st.error(\"Masukkan ID aplikasi Google Play.\")\n",
        "\n",
        "#**TAB 4: Preprocessing Data**\n",
        "with tab4:\n",
        "    st.header(\"Preprocessing Data\")\n",
        "    uploaded_file = st.file_uploader(\"Unggah file CSV untuk Preprocessing\", type=[\"csv\"])\n",
        "\n",
        "    if uploaded_file is not None:\n",
        "        # Baca file CSV\n",
        "        data = pd.read_csv(uploaded_file)\n",
        "\n",
        "        # Tampilkan preview data\n",
        "        st.write(\"📋 **Preview Data Sebelum Preprocessing**\")\n",
        "        st.dataframe(data.head(10))\n",
        "\n",
        "        if st.button('Mulai Preprocessing'):\n",
        "            with st.spinner('Proses preprocessing sedang berjalan...'):\n",
        "                preprocessed_data, tfidf_df = preprocess_data(data)\n",
        "                if preprocessed_data is not None:\n",
        "                    st.success('Preprocessing selesai!')\n",
        "                    st.write(\"📋 **Preview Data Setelah Preprocessing**\")\n",
        "                    st.dataframe(preprocessed_data.head(10))\n",
        "\n",
        "                    # Simpan file hasil preprocessing\n",
        "                    csv_file = preprocessed_data.to_csv(index=False)\n",
        "                    st.download_button(\n",
        "                        label=\"📁 Unduh Data Preprocessed Sebagai CSV\",\n",
        "                        data=csv_file,\n",
        "                        file_name='preprocessed_comments.csv',\n",
        "                        mime='text/csv',\n",
        "                    )\n",
        "\n",
        "                    st.write(\"📋 **Matriks TF-IDF (10 Baris Pertama)**\")\n",
        "                    st.dataframe(tfidf_df.head(10))\n",
        "\n",
        "                    # Simpan file hasil TF-IDF\n",
        "                    csv_tfidf_file = tfidf_df.to_csv(index=False)\n",
        "                    st.download_button(\n",
        "                        label=\"📁 Unduh Matriks TF-IDF Sebagai CSV\",\n",
        "                        data=csv_tfidf_file,\n",
        "                        file_name='tfidf_matrix.csv',\n",
        "                        mime='text/csv',\n",
        "                    )\n",
        "    else:\n",
        "        st.info(\"Silakan unggah file CSV dengan kolom 'content' dan 'score'.\")\n",
        "\n",
        "    st.header('📈 Hasil Preprocessing')\n",
        "\n",
        "    if 'preprocessed_data' in st.session_state:\n",
        "        st.write(\"📋 **Data Setelah Preprocessing**\")\n",
        "        preprocessed_data = st.session_state['preprocessed_data']\n",
        "        st.dataframe(preprocessed_data.head(10))\n",
        "\n",
        "        st.download_button(\n",
        "            label=\"📁 Unduh Data Preprocessed Sebagai CSV\",\n",
        "            data=preprocessed_data.to_csv(index=False),\n",
        "            file_name='preprocessed_comments.csv',\n",
        "            mime='text/csv',\n",
        "        )\n",
        "    else:\n",
        "        st.info(\"Silakan jalankan preprocessing di tab sebelumnya.\")\n",
        "\n",
        "#**TAB 5: Training Model\n",
        "with tab5:\n",
        "    st.header(\"Training Model\")\n",
        "    uploaded_file = st.file_uploader(\"Unggah file CSV hasil preprocessing\", type=[\"csv\"])\n",
        "    if uploaded_file is not None:\n",
        "        data = pd.read_csv(uploaded_file)\n",
        "        st.dataframe(data.head())\n",
        "        if st.button('Latih dan Uji Model'):\n",
        "            results = train_and_evaluate_model(data)\n",
        "            for model_name, result in results.items():\n",
        "                st.subheader(f'{model_name}')\n",
        "                st.write(f'**Akurasi:** {result[\"accuracy\"]:.2f}')\n",
        "                st.text(result['report'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9U3LWgtin86d"
      },
      "source": [
        "# **RUN STREAMLIT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDAvh7M9Miao",
        "outputId": "afcb8e60-7667-4b64-b804-a165bbf9a3c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: streamlit: command not found\n",
            "/bin/bash: line 1: streamlit: command not found\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K\u001b[1G\u001b[0JNeed to install the following packages:\n",
            "localtunnel@2.0.2\n",
            "Ok to proceed? (y) \u001b[20G^C\n"
          ]
        }
      ],
      "source": [
        "!streamlit cache clear\n",
        "!streamlit run app.py & npx localtunnel --port 8501"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
